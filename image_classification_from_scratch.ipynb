{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e680c21a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "\n",
    "##################################################\n",
    "#Device Setup\n",
    "##################################################\n",
    "DEVICE = torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\"); print(f\"Using device:{DEVICE}\")\n",
    "NUM_WORKERS = os.cpu_count(); print(f\"Number of CPU cores: {NUM_WORKERS}\")\n",
    "\n",
    "\n",
    "##################################################\n",
    "#Hyperparamaters\n",
    "##################################################\n",
    "BATCH_SIZE=32\n",
    "IMG_SIZE=224; INPUT_CHANNELS=3\n",
    "PATCH_SIZE=16\n",
    "NUM_HEADS = 12; MLP_SIZE=3072\n",
    "NUM_LAYERS=12\n",
    "EMBEDDING_DROPOUT, ATTENTION_DROPOUT, MLP_DROPOUT = 0.1, 0.1, 0.1\n",
    "NUM_CLASSES=200\n",
    "\n",
    "\n",
    "##################################################\n",
    "#Data Loading\n",
    "##################################################\n",
    "from torchvision.transforms import v2\n",
    "TRAIN_TRANSFORMATIONS = v2.Compose([\n",
    "    # Resize the image\n",
    "    v2.Resize((IMG_SIZE, IMG_SIZE), interpolation=v2.InterpolationMode.BILINEAR, antialias=True),\n",
    "\n",
    "    # Geometric Transformations\n",
    "    v2.RandomHorizontalFlip(p=0.5), # Random horizontal flip\n",
    "    v2.RandomRotation(degrees=15), # Random Rotation\n",
    "    v2.RandomPerspective(distortion_scale=0.2, p=0.5), # Random Perspective\n",
    "\n",
    "    # Photometric Transformations\n",
    "    v2.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1), # Random Color Jitter\n",
    "\n",
    "    # Convert to Tensors \n",
    "    v2.ToImage(), \n",
    "    v2.ToDtype(torch.float32, scale=True),\n",
    "    v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "print(f\"Training Transformations: {TRAIN_TRANSFORMATIONS}\")\n",
    "\n",
    "TEST_TRANSFORMATIONS = v2.Compose([\n",
    "    v2.Resize((IMG_SIZE, IMG_SIZE), interpolation=v2.InterpolationMode.BILINEAR, antialias=True),\n",
    "    v2.ToImage(), \n",
    "    v2.ToDtype(torch.float32, scale=True),\n",
    "    v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "print(f\"Validation/Test Transformations: {TEST_TRANSFORMATIONS}\")\n",
    "\n",
    "\n",
    "\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import DataLoader, random_split, Subset\n",
    "def create_train_val_dataloaders(root_dir:str, train_transformations:v2.Compose, val_transformations:v2.Compose, \n",
    "                       batch_size:int, num_workers:int, train_val_split:float=0.2):\n",
    "    \n",
    "    # Create two independent ImageFolder instances\n",
    "    train_full_dataset = datasets.ImageFolder(root=root_dir, transform=train_transformations)\n",
    "    val_full_dataset = datasets.ImageFolder(root=root_dir, transform=val_transformations)\n",
    "    class_names = train_full_dataset.classes; print(f\"Number of classes: {len(class_names)}\"); print(f\"Class names: {class_names}\")\n",
    "\n",
    "    total_samples = len(train_full_dataset); val_size = int(total_samples*train_val_split); train_size = total_samples-val_size\n",
    "    print(f\"Spitting dataset: Total={total_samples}, Train={train_size}, Validation={val_size}\")\n",
    "    \n",
    "    g = torch.Generator().manual_seed(42)\n",
    "    indices = torch.randperm(total_samples, generator=torch.Generator().manual_seed(42)).tolist()\n",
    "    train_indices = indices[:train_size]; val_indices=indices[train_size:]\n",
    "\n",
    "    train_subset = Subset(train_full_dataset, train_indices)\n",
    "    val_subset = Subset(val_full_dataset,val_indices)\n",
    "\n",
    "    train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "    val_loader = DataLoader(val_subset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "    return train_loader, val_loader, class_names\n",
    "\n",
    "\n",
    "ROOT_DIR = \"./archive/CUB_200_2011/images\"\n",
    "train_dataloader, val_dataloader, class_names = create_train_val_dataloaders(root_dir=ROOT_DIR, train_transformations=TRAIN_TRANSFORMATIONS, \n",
    "                                                                             val_transformations=TEST_TRANSFORMATIONS, batch_size=BATCH_SIZE,\n",
    "                                                                             num_workers=NUM_WORKERS, train_val_split=0.25)\n",
    "\n",
    "##################################################\n",
    "# Visualise the datasets\n",
    "##################################################\n",
    "from helper_functions import visualize_dataset\n",
    "visualize_dataset(dataset=train_dataloader, class_names=class_names, num_images=16, name=\"Training Dataset\", cols=4)\n",
    "visualize_dataset(dataset=val_dataloader, class_names=class_names, num_images=16, name=\"Validation Dataset\", cols=4)\n",
    "\n",
    "\n",
    "##################################################\n",
    "# Vision Transformer Architecture\n",
    "##################################################\n",
    "from model import VisionTransformer\n",
    "vit_model = VisionTransformer(image_size=IMG_SIZE, input_channels=INPUT_CHANNELS, patch_size=16, num_heads=12, mlp_size=3072, num_classes=NUM_CLASSES,\n",
    "                              num_layers=12, embedding_dropout=0.1, attention_dropout=0.1, mlp_dropout=0.1)\n",
    "\n",
    "from torchinfo import summary\n",
    "image_batch, label_batch = next(iter(train_dataloader))\n",
    "summary(model=vit_model, input_size=tuple(image_batch.shape),\n",
    "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"], row_settings=[\"var_names\"])\n",
    "\n",
    "##################################################\n",
    "# Optimizer & Loss function\n",
    "##################################################\n",
    "optimizer = torch.optim.Adam(lr=3e-3, params=vit_model.parameters(), betas=(0.9,0.999), weight_decay=0.3)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "##################################################\n",
    "# Training\n",
    "##################################################\n",
    "from helper_functions import set_seeds\n",
    "set_seeds()\n",
    "\n",
    "from engine import train\n",
    "history = train(model=vit_model, train_dataloader=train_dataloader, val_dataloader=val_dataloader, \n",
    "                loss_fn=loss_fn, optimizer=optimizer, epoch=10, device=DEVICE)\n",
    "\n",
    "\n",
    "##################################################\n",
    "# Plotting Training Curves\n",
    "##################################################\n",
    "from helper_functions import plot_loss_acc_curves\n",
    "plot_loss_acc_curves(results=history)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "<venv>",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
